[["classification.html", "Lab 4 Classification Overview 4.1 Unsupervised Classification 4.2 Supervised Classification 4.3 Accuracy Assessment 4.4 Hyperparameter Tuning 4.5 Additional Exercises Where to submit", " Lab 4 Classification Overview Although humans can look at a satellite image and identify objects or land cover types based on their visual characteristics, it’s typically both difficult and time consuming to do this consistently across large areas. To aid in this process, computers can employ a variety of machine learning techniques to more quickly evaluate patterns in the data and yield insights about what we can “see” in the imagery. Understanding image classification is an essential part of contemporary remote sensing, and opportunities abound for work in this growing field. Google Earth Engine offers many different types of classification approaches. At a high level, we can think of classification as a way of “predicting” labels to attach to a given series of pixels. In this lab we will explore two main types of classification - supervised and unsupervised, and you become more familiar with the processes of training data collection, classifier selection, classifier training, and accuracy assessment. At completion, you should be able to apply these skills to assess change in a given area. Within the context of image (land use) classification, let us define prediction as guessing the value of some geographic variable of interest g, using a function G that takes as input a pixel vector p: \\[ G_{t}(p_{i}) = g_{i} \\] The i in this equation refers to a particular instance from a set of pixels. Think of G as a guessing function and \\(g_{i}\\) as the guess for pixel i. The T in the subscript of G refers to a training set (a set of known values for p and the correct g), used to infer the structure of G. You have to choose a suitable G to train with T. When g is nominal, or a fixed category (ex., {'water', 'vegetation', 'bare'}), we call this classification. When g is numeric (ex., {0, 1, 2, 3}), we call this regression. This is a very generalized description of the types problem addressed in a broad range of fields including mathematics, statistics, data mining, machine learning. For our purposes, we will go through some examples using these concepts in Google Earth Engine and then provide more resources for further reading at the end. 4.1 Unsupervised Classification Unsupervised classification finds unique groupings in the dataset without the need for training data. The computer will cycle through the pixels, look at the characteristics of the different bands you instruct it to view, and pixel-by-pixel begin to group information together. Perhaps pixels with a blue hue and a low NIR value are grouped together, while green-dominant pixels are also grouped together. The outcome of unsupervised classification is that each pixel is categorized within the context of the image, and there will be the number of categories specified. One important note, is that the number of clusters is set by the user, and this plays a major role in how the algorithm operates. Too many clusters may create unnecessary noise, while too few clusters may not have sufficient granularity. Google Earth Engine provides documentation on working with unsupervised classification within their ecosystem, and we will be focusing on the ee.Clusterer package, which provides a flexible unsupervised classification (or clustering) in an easy-to-use way. The general workflow for clustering is as follows: Assemble features with numeric properties you want as inputs for your clusterer Instantiate a clusterer and set its parameters if necessary (e.g., number of clusters) “Train” (build/run) the clusterer model by indicating which data it should use Apply the clusterer to predict the values in an image or feature collection Assign a label the clusters Note that in this case, ‘train’ means to develop/run the model. Begin by creating a study region - in this case we will be working the Brazilian Amazon. // Lab: Unsupervised Classification (Clustering) // Create region var region = ee.Geometry.Polygon([[ [-54.07419968695418, -3.558053010380929], [-54.07419968695418, -3.8321399733300234], [-53.14310837836043, -3.8321399733300234], [-53.14310837836043, -3.558053010380929]]], null, false); Map.addLayer(region, {}, &quot;Region&quot;); Map.centerObject(region, 10); Now let’s add in a function to mask clouds and cloud shadows. // Function to mask clouds based on the pixel_qa band of Landsat 8 SR data. function maskL8sr(image) { // Bits 3 and 5 are cloud shadow and cloud, respectively. var cloudShadowBitMask = (1 &lt;&lt; 3); var cloudsBitMask = (1 &lt;&lt; 5); // Get the pixel QA band. var qa = image.select(&#39;pixel_qa&#39;); // Both flags should be set to zero, indicating clear conditions. var mask = qa.bitwiseAnd(cloudShadowBitMask).eq(0) .and(qa.bitwiseAnd(cloudsBitMask).eq(0)); return image.updateMask(mask); } Import Landsat 8 data, filter the data to the date range, apply the function to mask out pixels with clouds or cloud shadows rendering them unclear, and let’s start to work within the study region. // Load Landsat 8 annual composites. var landsat = ee.ImageCollection(&#39;LANDSAT/LC08/C01/T1_SR&#39;) .filterDate(&#39;2019-01-01&#39;, &#39;2019-12-31&#39;) .map(maskL8sr) .filterBounds(region) .median(); //Display Landsat data var visParams = { bands: [&#39;B4&#39;, &#39;B3&#39;, &#39;B2&#39;], min: 0, max: 3000, gamma: 1.4, }; Map.centerObject(region, 9); Map.addLayer(landsat, visParams, &quot;Landsat 8 (2016)&quot;); In this example, we will randomly sample 5000 pixels in the region to build a clustering model - we will use this ‘training’ data to find clustering groups and then apply it to predict values in the rest of the data. We will also set the variable clusterNum to identify how many categories to use. Start with 15, and modify based on the output and needs of your experiment. Note that we are using ee.Clusterer.wekaKMeans, which is a form of k-means clustering. In brief, k-means clustering is one of the simplest and most common unsupervised learning algorithms. The k-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible. // Create a training dataset. var training = landsat.sample({ region: region, scale: 30, numPixels: 5000 }); var clusterNum = 15 // Instantiate the clusterer and train it. var clusterer = ee.Clusterer.wekaKMeans(clusterNum).train(training); // Cluster the input using the trained clusterer. var result = landsat.cluster(clusterer); print(&quot;result&quot;, result.getInfo()); // Display the clusters with random colors. Map.addLayer(result.randomVisualizer(), {}, &#39;Unsupervised Classification&#39;); The resulting output shows a vivid display of colors. Under ‘Layers’, toggle the top-right of the map section and increase the transparency of the layer so as compare the classifed image to the natural color satellite imagery. Change the value of clusterNum and run through some different options to evaluate the impact of varying this input. Note that the output of an unsupervised clustering model is not specifying that each pixel should be a certain type of label (ex, the pixel is ‘water’), but rather that these pixels have similar characteristics. 1. If you were going to use a clustering model to identify water in the image, is 15 an appropriate cluster number? What process or features would you look for to determine what an appropriate cluster number would be? What do you see when you adjust the number of clusters in this region? 4.2 Supervised Classification Supervised classification is an iterative process of obtaining training data, creating an initial model, reviewing the results and tuning the parameters. Many projects using supervised classification may take several months of years of fine-tuning, requiring constant refinement and maintenance. Below is a list of the steps of Supervised Learning according to GEE’s documentation: Collect the training data Instantiate the classifier Train the classifier Classify the image Tune the model We will begin by creating training data manually within GEE. Using the geometry tools and the Landsat composite as a background, we can digitize training polygons. We’ll need to do two things: identify where polygons occur on the ground, and label them with the proper class number. Draw a polygon around an area of bare earth (dirt, no vegetation), then configure the import. Import as FeatureCollection, then click + New property. Name the new property ‘class’ and give it a value of 0. The dialog should show class: 0. Name the import ‘bare’. + New property &gt; Draw a polygon around vegetation &gt; import as FeatureCollection &gt; add a property &gt; name it ‘class’ and give it a value of 1. Name the import ‘vegetation’. + New property &gt; Draw a polygon around water &gt; import as FeatureCollection &gt; add a property &gt; name it ‘class’ and give it a value of 2. Name the import ‘water’. You should have three FeatureCollection imports named ‘bare’, ‘vegetation’ and ‘water’. Merge them into one FeatureCollection: var trainingFeatures = bare.merge(vegetation).merge(water); In the merged FeatureCollection, each Feature should have a property called ‘class’ where the classes are consecutive integers, one for each class, starting at 0. Verify that this is true. For Landsat, we will use the following bands for their predictive values - we could just keep the visual bands, but using a larger number of predictive values in many cases improves the model’s ability to find relationships and patterns in the data. var predictionBands = [&#39;B2&#39;, &#39;B3&#39;, &#39;B4&#39;, &#39;B5&#39;, &#39;B6&#39;, &#39;B7&#39;, &#39;B10&#39;, &#39;B11&#39;]; Create a training set T for the classifier by sampling the Landsat composite with the merged features. var classifierTraining = landsat.select(predictionBands) .sampleRegions({ collection: trainingFeatures, properties: [&#39;class&#39;], scale: 30 }); The choice of classifier is not always obvious, but a Classification and Regression Tree – often abbreviated as CART (a decision tree when running in classification mode) is an excellent starting point. Instantiate a CART and train it. var classifier = ee.Classifier.smileCart().train({ features: classifierTraining, classProperty: &#39;class&#39;, inputProperties: predictionBands }); Classify the image and display the output. var classified = landsat.select(predictionBands) .classify(classifier); Map.addLayer(classified, {min: 0, max: 2, palette: [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;]}, &#39;classified&#39;); Inspect the result. Some things to test if the result is unsatisfactory: Use other classifiers. Try some of the other classifiers in Earth Engine to see if the result is better or different. You can find different classifiers under Docs on the left panel of the console. Different (or more) training data. Try adjusting the shape and/or size of your training polygons to have a more representative sample of your classes. It is very common to either underfit or overfit your model when beginning the process. Add more predictors. For example, you can adding spectral indices to the input variables. 2. Pursue at least two of the three options above to adjust your supervised classification. Describe what you do and how it affects your results, providing visuals where useful to support your text. 4.3 Accuracy Assessment The previous section asked the question whether the result is satisfactory or not. In remote sensing, the quantification of the answer is called accuracy assessment. In the regression context, a standard measure of accuracy is the Root Mean Square Error (RMSE) or the correlation between known and predicted values. (Although the RMSE is returned by the linear regression reducer, beware: this is computed from the training data and is not a fair estimate of expected prediction error when guessing a pixel not in the training set). It is testing how accurate the model is based on the existing training data, but proper methodology uses separate ground-truth values for testing. In the classification context, accuracy measurements are often derived from a confusion matrix. The first step is to partition the set of known values into training and testing sets. Reusing the classification training set, add a column of random numbers used to partition the known data where about 60% of the data will be used for training and 40% for testing: var trainingTesting = classifierTraining.randomColumn(); var trainingSet = trainingTesting.filter(ee.Filter.lessThan(&#39;random&#39;, 0.6)); var testingSet = trainingTesting.filter(ee.Filter.greaterThanOrEquals(&#39;random&#39;, 0.6)); Train the classifier with the trainingSet: var trained = ee.Classifier.smileCart().train({ features: trainingSet, classProperty: &#39;class&#39;, inputProperties: predictionBands }); Classify the testingSet and get a confusion matrix. Note that the classifier automatically adds a property called ‘classification’, which is compared to the ‘class’ property added when you imported your polygons: var confusionMatrix = ee.ConfusionMatrix(testingSet.classify(trained) .errorMatrix({actual: &#39;class&#39;, predicted: &#39;classification&#39;})); Print the confusion (error) matrix and expand the object to inspect the matrix. The entries represent the number of pixels. Items on the diagonal represent correct classification. Items off the diagonal are misclassifications, where the class in row i is classified as column j. It’s also possible to get basic descriptive statistics from the confusion matrix. For example: print(&#39;Confusion matrix:&#39;, confusionMatrix); print(&#39;Overall Accuracy:&#39;, confusionMatrix.accuracy()); print(&#39;Producers Accuracy:&#39;, confusionMatrix.producersAccuracy()); print(&#39;Consumers Accuracy:&#39;, confusionMatrix.consumersAccuracy()); Note that you can test different classifiers by replacing CART with some other classifier of interest. Also note that as a result of the randomness in the partition, you may get different results from different runs. 3. Upload your confusion (error) matrix for your preferred classification and describe the content of the table in terms of overall accuracy as well as user and producer error. 4.4 Hyperparameter Tuning Another fancy classifier is called a random forest (Breiman 2001). A random forest is a ensemble model consisting of many decision trees that only contain a random selection of the input data. The simple logic underpinning random forests is that individual predictions from decision trees may not be accurate, but once combined, the average predictions from the trees will more closely approximate the truth. Random forests can be used for both computing averages (regression) or determining labels (classification), and they rank among some of the most common as well as high-performing classification approahces. Because random forests tend to perform well, we need to make things a little harder for this exercise to be interesting. Let’s do that by adding noise to the training data: var sample = landsat.select(predictionBands).sampleRegions( {collection: trainingFeatures .map(function(f) { return f.buffer(300) } ), properties: [&#39;class&#39;], scale: 30}); var classifier = ee.Classifier.smileRandomForest(10) .train({features: sample, classProperty: &#39;class&#39;, inputProperties: predictionBands }); var classified = landsat.select(predictionBands).classify(classifier); Map.addLayer(classified, {min: 0, max: 2, palette: [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;]}, &#39;classified&#39;) Note that the only parameter to the classifier is the number of trees (10). How many trees should you use? Making that choice is often done through a process of choosing ‘optimal’ model arguments prior to running the learning algorithm – the process often called hyperparameter tuning. For example, var sample = sample.randomColumn(); var train = sample.filter(ee.Filter.lt(&#39;random&#39;, 0.6)); var test = sample.filter(ee.Filter.gte(&#39;random&#39;, 0.6)); var numTrees = ee.List.sequence(5, 50, 5); var accuracies = numTrees.map(function(t) { var classifier = ee.Classifier.smileRandomForest(t) .train({ features: train, classProperty: &#39;class&#39;, inputProperties: predictionBands }); return test.classify(classifier) .errorMatrix(&#39;class&#39;, &#39;classification&#39;) .accuracy(); }); print(ui.Chart.array.values({ array: ee.Array(accuracies), axis: 0, xLabels: numTrees })); You should see something like the following chart, in which the number of trees is on the x-axis and estimated accuracy is on the y-axis: First, note that we always get very high accuracy in this simple example. Second, note adding more trees eventually doesn’t yield much more accuracy for the increased computational burden we impose. 3. Upload the resulting graphic and interpret its output. 4.5 Additional Exercises Design a four-class classification for your area of interest. Decide on suitable input data and manually collect training points (or polygons) if necessary. Tune a random forest classifier. In your code, have a variable called trees that sets the optimal number of trees according to your hyperparameter tuning process. Have a variable called maxAccuracy that stores the estimated accuracy for the optimal number of trees. 4. Upload your resulting classified image and describe the location and labels. Provide the confusion/error matrix for this region, the optimal number of trees, and maxAccuracy for the optimal number of trees. Are you satisfied with the classification output? Why or why not? Where to submit Submit your responses to these questions on Gradescope by 10am on Wednesday, September 30. If needed, the access code for our course is 6PEW3W. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
